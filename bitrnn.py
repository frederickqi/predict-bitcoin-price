# -*- coding: utf-8 -*-
"""bitRNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cblqKf6RZz50pRVksH_a9znTJyDndOTr
"""

! pip install vaderSentiment pandas_datareader textblob

"""# Read and clean the data 

"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import pandas_datareader as web
import datetime as dt
import io
import os
import re
import io
import nltk
import json
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
from tqdm import tnrange, tqdm_notebook, tqdm
from sklearn import preprocessing
from time import sleep
from nltk.tokenize import TweetTokenizer
from nltk.stem.wordnet import WordNetLemmatizer
from textblob import TextBlob
import pandas_datareader as web
import datetime as dt
from sklearn.preprocessing import MinMaxScaler

# crypto_currency = 'ETH'
# against_currency = 'USD'
start = dt.datetime(2022, 2, 6)
# end = dt.datetime.now()
# end = dt.datetime(2022, 7, 30)
# data = web.DataReader(f'{crypto_currency}-{against_currency}', 'yahoo', start, end)
# tweets_raw_file   = '/kaggle/input/bitcoin-tweets/Bitcoin_tweets.csv'
tweets_clean_file = './Bitcoin_tweets_clean.csv'
price_data = './2021M_2022J.csv'

for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

#Data Cleaning process from the following:

# df_raw = pd.read_csv(tweets_raw_file,low_memory=False)
# print(df_raw.shape)
# df_raw.head(5)
# df_raw = df_raw.sort_values(by = 'date')
# dd = df_raw.sample(frac=0.01, replace=False, random_state=1)
# dd.reset_index(inplace=True)
# for i,s in enumerate(tqdm(dd['text'],position=0, leave=True)):
#     text = str(dd.loc[i, 'text'])
#     text = text.replace("#", "")
#     text = re.sub('https?://(?:[-\w.]|(?:%[\da-fA-F]{2}))+', '', text, flags=re.MULTILINE)
#     text = re.sub('@\\w+ *', '', text, flags=re.MULTILINE)
#     dd.loc[i, 'text'] = text
# # f = open(tweets_clean_file, 'a+', encoding='utf-8')
# dd.to_csv(tweets_clean_file, header=True, encoding='utf-8',index=False)

df_clean = pd.read_csv(tweets_clean_file)

analyzer = SentimentIntensityAnalyzer()
compound = []
for i,s in enumerate(tqdm(df_clean['text'],position=0, leave=True)):
    # print(i,s)
    vs = analyzer.polarity_scores(str(s))
    compound.append(vs["compound"])
df_clean["compound"] = compound
df_clean.head(10)

scores = []
for i, s in tqdm(df_clean.iterrows(), total=df_clean.shape[0],position=0, leave=True):
    try:
        scores.append(s["compound"] * ((int(s["user_followers"]))) * ((int(s["user_favourites"])+1)/int(s['user_followers']+1)) *((int(s["is_retweet"])+1)))
    except:
        scores.append(np.nan)
df_clean["score"] = scores
df_clean.head(10)

"""# Sentiment Analysis

"""

data = pd.read_csv(price_data)
df_clean = df_clean.drop_duplicates()
tweets = df_clean.copy()
tweets['date'] = pd.to_datetime(tweets['date'],utc=True)
tweets.date = tweets.date.dt.tz_localize(None)
tweets.index = tweets['date']
tweets_grouped = tweets.resample('1h').sum()
crypto_usd = data.copy()
# data.head(10)
# crypto_usd = pd.to_datetime(crypto_usd)
crypto_usd['Date'] = pd.to_datetime(crypto_usd['Date'])
crypto_usd.index = crypto_usd['Date']
# crypto_usd['Timestamp'] = pd.to_datetime(crypto_usd['Timestamp'], unit='s')
# crypto_usd.index = crypto_usd['Timestamp']
# crypto_usd_grouped = crypto_usd.groupby(pd.TimeGrouper('1h'))['Weighted_Price'].mean()
crypto_usd_grouped = crypto_usd.resample('D')['Close'].mean()
# crypto_usd_grouped = crypto_usd.resample('D', on='Close').mean()
crypto_usd_grouped.head(10)

def crosscorr(datax, datay, lag=0, method="pearson"):
  return datax.corrwith(datay.shift(lag), method=method)['score']

beggining = max(tweets_grouped.index.min().replace(tzinfo=None), crypto_usd_grouped.index.min())
end = min(tweets_grouped.index.max().replace(tzinfo=None), crypto_usd_grouped.index.max())
tweets_grouped = tweets_grouped[beggining:end]
crypto_usd_grouped = crypto_usd_grouped[beggining:end]

fig, ax1 = plt.subplots(figsize=(20,10))
ax1.set_title("Bitcoin Price V.S Sentiment on Twitter", fontsize=18)
ax1.tick_params(labelsize=14)
ax2 = ax1.twinx()
ax1.plot_date(tweets_grouped.index, tweets_grouped, 'hotpink')
ax2.plot_date(crypto_usd_grouped.index, crypto_usd_grouped, 'teal')


ax1.set_ylabel("Sentiment", color='hotpink', fontsize=15)
ax1.set_xlabel("Date", fontsize=16)
ax2.set_ylabel("Bitcoin Price $s", color='teal', fontsize=15)


plt.show()
plt.savefig('crypto_evolution_vs_twitter_sentiment', dpi=1000)

xcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i, method="pearson") for i in range(-20,20)]
plt.plot(range(-20,20), xcov)
plt.title("pearson cross-correlation")
plt.xlabel("lag")
plt.ylabel("correlation")
plt.show()

xcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i, method="kendall") for i in range(-20,20)]
plt.plot(range(-20,20), xcov)
plt.title("kendall cross-correlation")
plt.xlabel("lag")
plt.ylabel("correlation")
plt.show()

xcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i, method="spearman") for i in range(-20,20)]
plt.plot(range(-20,20), xcov)
plt.title("spearman cross-correlation")
plt.xlabel("lag")
plt.ylabel("correlation")
plt.show()

"""# Normalizaon of scores

"""

min_max_scaler = preprocessing.StandardScaler()
score_scaled = min_max_scaler.fit_transform(tweets_grouped['score'].values.reshape(-1,1))
tweets_grouped['normalized_score'] = score_scaled
crypto_used_grouped_scaled = crypto_usd_grouped / max(crypto_usd_grouped.max(), abs(crypto_usd_grouped.min()))

fig, ax1 = plt.subplots(figsize=(20,10))
ax1.set_title("Normalized Bitcoin Price V.S Normalized Sentiment on Twitter", fontsize=18)
ax1.tick_params(labelsize=14)

ax2 = ax1.twinx()
ax1.plot_date(tweets_grouped.index, tweets_grouped['normalized_score'], 'hotpink')
ax2.plot_date(crypto_usd_grouped.index, crypto_used_grouped_scaled, 'teal')

ax1.set_ylabel("Sentiment", color='hotpink', fontsize=16)
ax1.set_xlabel("Time period", fontsize=16)
ax2.set_ylabel("Bitcoin normalized", color='teal', fontsize=16)
plt.show()
plt.savefig('normalized_crypto_evolution_vs_twitter_sentiment', dpi=1000)

xcov = [crosscorr(tweets_grouped, crypto_usd_grouped, lag=i) for i in range(-20,20)]
plt.plot(range(-20,20), xcov)
plt.title("lag correcleation (updated)")
plt.xlabel("lag")
plt.ylabel("correlation")
plt.show()



"""# NLP-related data cleaning"""

df = df_clean.copy()
df.dropna(subset=['hashtags'], inplace=True)
df = df[['text']] 
df.columns = ['tweets']
df.head()

nltk.download('omw-1.4')
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')
stopWords = nltk.corpus.stopwords.words(['english'])

lem = WordNetLemmatizer()

#Designed for cleaning the data by removing hashtages,mentions, stopwards, etc.

def cleaning(data):
    tweet_without_url = re.sub(r'http\S+',' ', data)
    tweet_without_hashtag = re.sub(r'#\w+', ' ', tweet_without_url)
    tweet_without_mentions = re.sub(r'@\w+',' ', tweet_without_hashtag)
    precleaned_tweet = re.sub('[^A-Za-z]+', ' ', tweet_without_mentions)
    tweet_tokens = TweetTokenizer().tokenize(precleaned_tweet)
    tokens_without_punc = [w for w in tweet_tokens if w.isalpha()]
    tokens_without_sw = [t for t in tokens_without_punc if t not in stopWords]
    text_cleaned = [lem.lemmatize(t) for t in tokens_without_sw]
    return " ".join(text_cleaned)

df['cleaned_tweets'] = df['tweets'].apply(cleaning)
df['date'] = df_clean['date']
df['date_clean'] = pd.to_datetime(df['date']).dt.strftime('%Y-%m-%d')
df.drop(columns='date',inplace=True)
df.head()

def crypto_price_cate(score):
    if score < 1:
        return 'negative'
    elif score == 1:
        return 'neutral'
    else:
        return 'positive'

def observe_period(period):
    res = crypto_usd_grouped.shift(period)/crypto_usd_grouped
    res = res.apply(crypto_price_cate)
    return res

def getPolarity(tweet):
    return TextBlob(tweet).sentiment.polarity

def getSubjectivity(tweet):
    return TextBlob(tweet).sentiment.subjectivity

#This block is designed for measuring the changes of sentiment score over 7 days.
time_sentiment = observe_period(7)
df['crypto_sentiment'] = df.date_clean.apply(lambda x: time_sentiment[x] if x in time_sentiment else np.nan)
df['subjectivity'] = df['cleaned_tweets'].apply(getSubjectivity)
df['polarity'] = df['cleaned_tweets'].apply(getPolarity)
df.head()

# This block is designed for loading the data to a new CSV
def getSentiment(score):
    if score < 0:
        return 'negative'
    elif score == 0:
        return 'neutral'
    else:
        return 'positive'
df['sentiment'] = df['polarity'].apply(getSentiment)
df['target'] = df['sentiment'] == df['crypto_sentiment']
df.head()
df.to_csv('./df_data.csv')

import seaborn as sns
plt.figure(figsize=[16,9])
sns.countplot(df.crypto_sentiment)
plt.title('Sentiment Counts', fontsize=15)

label = (df.crypto_sentiment.value_counts(normalize=True)*100).round(2)
for i in range(2):
    plt.text(x = i, y = label[i], s = label[i],horizontalalignment='center',rotation = 360, color = "red", weight="bold", fontsize=15)

plt.legend
plt.show()
plt.savefig('sentiment_vs_count', dpi=1000)



"""# Model Implementation"""

from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Bidirectional, SpatialDropout1D
from tensorflow.keras.models import load_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.metrics import confusion_matrix
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.losses import SparseCategoricalCrossentropy
from tensorflow.keras.optimizers import Adam
import tensorflow as tf
import tensorflow.keras.layers as Layers
import tensorflow.keras.backend as K
import seaborn as sns
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras import datasets
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.callbacks import History
from tensorflow.keras import losses
from tensorflow.keras.preprocessing import sequence
import tensorflow.keras.backend as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Embedding,Conv1D,MaxPooling1D,LSTM
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report
from sklearn.metrics import confusion_matrix
import seaborn as sns

X = df['cleaned_tweets']
y = pd.get_dummies(df['sentiment']).values
num_classes = df['sentiment'].nunique()
seed = 40
np.random.seed(seed)
X_train, X_test, y_train, y_test = train_test_split(X, y, 
                                                    test_size=0.2,
                                                    stratify=y,
                                                    random_state=seed)


max_features = 20000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(X_train))
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)
max_words = 30
X_train = sequence.pad_sequences(X_train, maxlen=max_words)
X_test = sequence.pad_sequences(X_test, maxlen=max_words)
print(X_train.shape,X_test.shape)

batch_size = 128
epochs = 10
max_features = 20000
embed_dim = 100
np.random.seed(seed)
K.clear_session()
model = Sequential()
model.add(Embedding(max_features, embed_dim, input_length=X_train.shape[1]))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))    
model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))
model.add(Dense(num_classes, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
model.summary()
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), 
                          epochs=epochs, batch_size=batch_size, verbose=2)
tf.keras.utils.plot_model(model, show_shapes=True)

def plot_training_hist(history):
    '''Function to plot history for accuracy and loss'''
    
    fig, ax = plt.subplots(1,2, figsize=(10,4))
    ax[0].plot(history.history['accuracy'])
    ax[0].plot(history.history['val_accuracy'])
    ax[0].set_title('Model Accuracy')
    ax[0].set_xlabel('epoch')
    ax[0].set_ylabel('accuracy')
    ax[0].legend(['Train', 'Validation'], loc='best')
    
    ax[1].plot(history.history['loss'])
    ax[1].plot(history.history['val_loss'])
    ax[1].set_title('Model Loss')
    ax[1].set_xlabel('epoch')
    ax[1].set_ylabel('loss')
    ax[1].legend(['train', 'validation'], loc='best')
    
    plt.savefig("train_accuracy_loss", dpi=1000)

y_pred_test =  np.argmax(model.predict(X_test), axis=1)
print('Accuracy:\t{:0.1f}%'.format(accuracy_score(np.argmax(y_test,axis=1),y_pred_test)*100))
print(classification_report(np.argmax(y_test,axis=1), y_pred_test))

def plot_confusion_matrix(model, X_test, y_test):
    
    sentiment_classes = ['Negative','Neutral', 'Positive']
    y_pred = model.predict(X_test)
    cm = confusion_matrix(np.argmax(y_pred, axis=1),np.argmax(np.array(y_test),axis=1))
    print(pd.Series(np.argmax(np.array(y_test),axis=1)).value_counts())
    print(pd.Series(np.argmax(y_pred, axis=1)).value_counts())
    plt.figure(figsize=(10,10))
    sns.heatmap(cm, cmap=plt.cm.Blues, annot=True, fmt='d', 
                xticklabels=sentiment_classes,
                yticklabels=sentiment_classes)
    plt.title('Confusion matrix', fontsize=16)
    plt.xlabel('Actual label', fontsize=12)
    plt.ylabel('Predicted label', fontsize=12)
    plt.savefig("train_matrix", dpi=1000)
    
plot_confusion_matrix(model, X_test, y_test)



"""# Test the LSTM model"""

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow.keras.backend as K
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Bidirectional, LSTM, Dense, Dropout
from tensorflow.keras.metrics import Precision, Recall
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras import datasets
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.callbacks import History
from tensorflow.keras import losses
max_words = 5000
max_len=50

def tokenize_pad_sequences(text):
    tokenizer = Tokenizer(num_words=max_words, lower=True, split=' ')
    tokenizer.fit_on_texts(text)
    X = tokenizer.texts_to_sequences(text)
    X = pad_sequences(X, padding='post', maxlen=max_len)
    return X, tokenizer
X, tokenizer = tokenize_pad_sequences(df['cleaned_tweets'])
y = pd.get_dummies(df['sentiment'])
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)

def f1_score(precision, recall):
    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    return f1_val

vocab_size = 5000
embedding_size = 32
epochs = 50
learning_rate = 0.01
decay_rate = learning_rate / epochs
momentum = 0.8
sgd = SGD(learning_rate=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)
model= Sequential()
model.add(Embedding(vocab_size, embedding_size, input_length=max_len))
model.add(Conv1D(filters=32, kernel_size=1, padding='same', activation='relu'))
model.add(MaxPooling1D(pool_size=2))
model.add(Bidirectional(LSTM(32)))
model.add(Dropout(0.4))
model.add(Dense(3, activation='softmax'))
tf.keras.utils.plot_model(model, show_shapes=True)

model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy', Precision(), Recall()])
history = model.fit(X_train,y_train,validation_data=(X_val, y_val),batch_size=batch_size,epochs=epochs,verbose=1)

plot_training_hist(history)

"""# Data Load and price prediction

"""

! pip install pandas_datareader

# crypto_currency = 'BTC'
# against_currency = 'USD'
# start = dt.datetime(2016, 1, 1)
# end = dt.datetime.now()
# data = web.DataReader(f'{crypto_currency}-{against_currency}', 'yahoo', start, end)
from sklearn.preprocessing import MinMaxScaler
import pandas as pd
import numpy as np
import pandas_datareader as web
import datetime as dt

# start = dt.datetime(2017,11, 10)
# end = dt.datetime(2022, 10, 30)
price_data = './lastThreeDays.csv'
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data['Close'].values.reshape(-1, 1))

preddays = 10

x_train, y_train = [], []
for x in range(preddays, len(scaled_data)):
    x_train.append(scaled_data[x-preddays:x, 0])
    y_train.append(scaled_data[x, 0])
    
x_train, y_train = np.array(x_train), np.array(y_train)
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))
model = Sequential()
model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))
model.add(Dropout(0.2))
model.add(LSTM(units=50, return_sequences=True))
model.add(Dropout(0.2))
model.add(LSTM(units=50))
model.add(Dropout(0.2))
model.add(Dense(units = 1))
model.compile(optimizer='adam', loss='mean_squared_error')
history = model.fit(x_train, y_train, epochs=50, batch_size=32, verbose=1)

test_start = dt.datetime(2020,11,5)
test_end= dt.datetime(2022, 11, 5)
print(test_start)
print(test_end)
print(data.index)
data.head(10)
data["Date"] = pd.to_datetime(data["Date"])
test_data = data.loc[(data["Date"] >= test_start) & (data["Date"] <= test_end)]
actual_prices = test_data['Close'].values
total_dataset = pd.concat((data['Close'], test_data['Close']), axis=0)

model_inputs = total_dataset[len(total_dataset) - len(test_data) - preddays:].values
model_inputs = model_inputs.reshape(-1, 1)
model_inputs = scaler.fit_transform(model_inputs)
x_test = []
for x in range(preddays,  len(model_inputs)):
    x_test.append(model_inputs[x-preddays:x, 0])   
x_test = np.array(x_test)
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

pred = model.predict(x_test)
pred = scaler.inverse_transform(pred)

plt.figure(figsize=(16,9))
plt.plot(actual_prices, color="hotpink", label='Actual prices of BitCoin')
plt.plot(pred, color='teal', label='Our Predicted Prices')
plt.title(f'Real BitCoin Price Prediction with the Model')
plt.xlabel('Time Span')
plt.ylabel('Price of Bitcoin ($)')
plt.legend(loc='upper right')
plt.show()
plt.savefig("btc_price_prediction", dpi=1500)

